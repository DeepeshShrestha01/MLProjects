{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50003b14-15c9-4f64-b0e7-efd91a4c5408",
   "metadata": {},
   "source": [
    "IMPORTING THE REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f4e09b-7816-4813-a12f-21546b46d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0179fb7-9e45-475e-afd6-310ee685e08c",
   "metadata": {},
   "source": [
    "READING THE XML FILES PATH FROM THE CSV FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c925e321-81e1-419d-8d0e-83307641e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Labels.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac2e2e-74ed-4de7-b74a-88735fe6a596",
   "metadata": {},
   "source": [
    "PARSING THE XML FILES TO GET THE IMAGE NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2b0798-fb85-4089-801b-fe4b41c77147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image name is stored as the <filename> \n",
    "import xml.etree.ElementTree as xet\n",
    "#Let's create a function to parse the image name and the generate the image path\n",
    "def getImagePath(filename):\n",
    "    filename_image = xet.parse(filename).getroot().find('filename').text # Parse the imagename from the xml\n",
    "    filepath_image = os.path.join('./img',filename_image) #Join the image name with the path to generate complete path for the image\n",
    "    return filepath_image\n",
    "#Let's appply the getImagePath function to the entire lsit of xml files to generate the list of image paths\n",
    "image_path = list(df['filepath'].apply(getImagePath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07914763-178a-47f9-9b34-f82fd3690130",
   "metadata": {},
   "source": [
    "VERIFYING THAT IMAGE READING CAN BE DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764f376b-b3b8-4457-b3fd-7158a4c5b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = image_path[0]\n",
    "#Using the OpenCV module to read image in BGR format\n",
    "img = cv2.imread(filepath)\n",
    "# Let's see the boxed window for the img region to be read\n",
    "cv2.rectangle(img,(228,130),(425,171), (0,255,0), 3)\n",
    "# Create a window named \"example\" and set it's size as WINDOW_NORMAL\n",
    "cv2.namedWindow('example',cv2.WINDOW_NORMAL) \n",
    "cv2.imshow('example',img)\n",
    "cv2.waitKey(0) #Close the window on click of any key\n",
    "cv2.destroyAllWindows() #Close all the windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b96d2-c3de-4fc5-969c-6928386b5cfe",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1167e497-21c4-4da0-9f54-9daf8ab3bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "data = [] #Creating list for storing the array of images as elements\n",
    "output = [] #Creating list for storing the bouding boxes \n",
    "labels = df.iloc[:,1:].values #Collect the dimension of the red box for OCR in DataFrame labels\n",
    "for ind in range (len(image_path)): #For all the images\n",
    "    image = image_path[ind] # Transfer each image path to the image variable\n",
    "    img_arr = cv2.imread(image) # read the image in BGR format using OpenCV \n",
    "    h,w,d = img_arr.shape # Dimension of the image in numbers\n",
    "    # Performing the Preprocessing\n",
    "    load_image = load_img(image, target_size=(224,224)) # Load the image from the 'image' file path and resize it to a target size of 224x224 pixels\n",
    "    load_image_arr = img_to_array(load_image) # Convert the loaded image (in PIL format) to a NumPy array\n",
    "    # Performing Normalization on hte load_image_arr\n",
    "    norm_load_image_arr = load_image_arr/255.0\n",
    "    # Performing the Normalization on the boundbox coordinates\n",
    "    xmin,xmax,ymin,ymax = labels[ind] #Tuple Unpacking done in numpy array labels Can't be done in DataFrame\n",
    "    nxmin,nxmax = xmin/w, xmax/w #normalizing with respect to the width\n",
    "    nymin,nymax = ymin/h, ymax/h #normalizing with respect to the height\n",
    "    label_norm = (nxmin, nxmax, nymin, nymax) #Tuple creation for the bouding box with normalized dimensions\n",
    "    data.append(norm_load_image_arr)\n",
    "    output.append(label_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df0c3d-352e-4791-b7ac-1ce2a2612bbb",
   "metadata": {},
   "source": [
    "CONVERTING THE data AND output LIST INTO NUMPY ARRAY AND ASSIGNING THEM TO X AND y\n",
    "<br>TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9b304d-0ec3-4345-949a-ca0127139416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data, dtype=np.float32)\n",
    "y = np.array(output, dtype=np.float32)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size= 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0e339-fecf-46c2-a01c-ddd276f64f35",
   "metadata": {},
   "source": [
    "DEEP LEARNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a5325ab-b2ef-4102-8aba-cf8a6b1670f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2, InceptionV3, InceptionResNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a65a68e3-1bdc-4eb4-b1d2-09c0e40a156e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inception_resnet = InceptionResNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "inception_resnet.trainable = False\n",
    "headmodel = inception_resnet.output\n",
    "headmodel = Flatten()(headmodel)\n",
    "headmodel = Dense(500, activation=\"relu\")(headmodel)\n",
    "headmodel = Dense(250, activation=\"relu\")(headmodel)\n",
    "headmodel = Dense(4, activation='sigmoid')(headmodel)\n",
    "model = Model(inputs=inception_resnet.input, outputs=headmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f15cd-0e81-488e-9d76-e3cbf2ddba0a",
   "metadata": {},
   "source": [
    "COMPILING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b44b357-6b33-43f4-8acf-86c9603abe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-40))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d08cf6-8c95-4521-bb7d-03b459db0ae8",
   "metadata": {},
   "source": [
    "TRAINING THE RESNETV2 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39df4ad8-a3b5-43a6-ad82-7b60fe94a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "265ef2de-496b-4385-b519-28a469f58953",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfb = TensorBoard('object_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3373e89-c56f-4d66-bb04-3aea78fae55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - loss: 0.1178 - val_loss: 0.1385\n",
      "Epoch 2/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - loss: 0.1112 - val_loss: 0.1385\n",
      "Epoch 3/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - loss: 0.1050 - val_loss: 0.1385\n",
      "Epoch 4/100\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 1s/step - loss: 0.1229"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, y=y_train, batch_size=10, epochs=100, validation_data=(X_test,y_test), callbacks=[tfb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a8fab-433e-4388-9f5d-bd25071108c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train, y=y_train, batch_size=10, epochs=200, validation_data=(X_test,y_test), callbacks=[tfb], initial_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00740edd-8ae8-4564-bf1f-990956681b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/object_detection.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0692d2b-bf4d-4424-be45-64fe926f541c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
